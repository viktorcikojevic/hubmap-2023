{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.158863Z",
     "iopub.status.busy": "2023-07-11T20:55:50.158344Z",
     "iopub.status.idle": "2023-07-11T20:55:50.649519Z",
     "shell.execute_reply": "2023-07-11T20:55:50.649187Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.append(\"detection-wheel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.651038Z",
     "iopub.status.busy": "2023-07-11T20:55:50.650891Z",
     "iopub.status.idle": "2023-07-11T20:55:50.652551Z",
     "shell.execute_reply": "2023-07-11T20:55:50.652375Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.653765Z",
     "iopub.status.busy": "2023-07-11T20:55:50.653677Z",
     "iopub.status.idle": "2023-07-11T20:55:51.175336Z",
     "shell.execute_reply": "2023-07-11T20:55:51.175011Z"
    }
   },
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "from skimage.draw import polygon\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs, masks, mode='train'):\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = imgs\n",
    "        self.masks = masks\n",
    "\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "        labels_file = \"/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/polygons.jsonl\"\n",
    "        with open(labels_file, 'r') as json_file:\n",
    "            self.json_labels = [json.loads(line) for line in json_file]\n",
    "            \n",
    "        # get index for each 'id' \n",
    "        indices_map = {}\n",
    "        for indx, label in enumerate(self.json_labels):\n",
    "            indices_map[label['id']] = indx\n",
    "        \n",
    "        self.indices_map = indices_map\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.alb_transform = A.Compose([\n",
    "                \n",
    "                \n",
    "                    A.Rotate(limit=90, p=0.9),\n",
    "                    A.ShiftScaleRotate(p=0.5),\n",
    "                    A.RandomBrightnessContrast(p=0.2),\n",
    "                    A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.2),\n",
    "                    A.RandomGamma(p=0.2),\n",
    "                    # A.RandomCrop(height=256, width=256, p=0.5),\n",
    "                    A.RandomResizedCrop(height=512, width=512, p=0.5),\n",
    "                    A.Affine(p=0.5),\n",
    "                    A.Downscale(scale_min=0.1, scale_max=0.5, p=0.5),\n",
    "                    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, mask_fill_value=0, p=0.3),\n",
    "                    A.ShiftScaleRotate(shift_limit=0, scale_limit=0, p=0.3),\n",
    "                    \n",
    "                    \n",
    "                    A.Normalize(\n",
    "                        mean= [0] * 3,\n",
    "                        std= [1] * 3\n",
    "                    ),\n",
    "                \n",
    "                \n",
    "                    ToTensorV2(transpose_mask=True),\n",
    "                ])\n",
    "            \n",
    "            self.random_zoom_out = T.Compose([T.RandomZoomOut(p=0.8)])\n",
    "        else:\n",
    "            self.alb_transform = A.Compose([\n",
    "                    \n",
    "                    A.Normalize(\n",
    "                        mean= [0] * 3,\n",
    "                        std= [1] * 3\n",
    "                    ),\n",
    "                \n",
    "                \n",
    "                    ToTensorV2(transpose_mask=True),\n",
    "                ])\n",
    "            \n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        num_objs = 0\n",
    "        while num_objs == 0:\n",
    "        \n",
    "            idx = np.random.randint(0, len(self.imgs))\n",
    "            \n",
    "            # load images and masks\n",
    "            img_path = self.imgs[idx]\n",
    "            mask_path = self.masks[idx] # '/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/0006ff2aa7cd_mask.png'\n",
    "            \n",
    "            \n",
    "            \n",
    "            # load image\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = np.array(img)\n",
    "            \n",
    "            # load mask, but first get the id\n",
    "            mask_id = mask_path.split('/')[-1].split('_')[0]\n",
    "            \n",
    "            mask = np.zeros((512, 512), dtype=np.float32)\n",
    "            \n",
    "            \n",
    "            mask_id_indx = self.indices_map[mask_id]\n",
    "            for annot in self.json_labels[mask_id_indx]['annotations']:\n",
    "                cords = annot['coordinates']\n",
    "                if annot['type'] == \"blood_vessel\":\n",
    "                    for cord in cords:\n",
    "                        rr, cc = polygon(np.array([i[1] for i in cord]), np.asarray([i[0] for i in cord]))\n",
    "                        mask[rr, cc] = 1\n",
    "            \n",
    "        \n",
    "            # ---------------- Augmentations ----------------\n",
    "            \n",
    "            \n",
    "            \n",
    "            # albumentations\n",
    "            transformed = self.alb_transform(image=img, mask=mask)\n",
    "            \n",
    "            img = transformed['image']\n",
    "            mask = transformed['mask'].numpy()\n",
    "            \n",
    "            # return img, mask\n",
    "            \n",
    "            # print(img.shape, mask.shape)\n",
    "            mask_uint8 = np.where(mask > 0.5, 1, 0).astype(np.uint8) * 255\n",
    "                \n",
    "            num_outputs, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_uint8)\n",
    "            label_masks = [labels == i for i in range(num_outputs)]\n",
    "            masks = []\n",
    "            mask_areas = []\n",
    "            for m in label_masks:\n",
    "                mask_m = mask * m\n",
    "                mask_area = np.sum(mask_m)\n",
    "                if mask_area > 10:\n",
    "                    masks.append(mask_m)\n",
    "                    mask_areas.append(mask_area)\n",
    "                    \n",
    "            \n",
    "            # Duplicate augmentations. For the 2 smallest masks, do the following:\n",
    "            # i) randomly np.roll them in x and y direction\n",
    "            # ii) randomly flip them in x and y direction\n",
    "            # add them to the masks list\n",
    "            \n",
    "            smallest_masks = np.argsort(mask_areas)[:10]\n",
    "            \n",
    "            # take random 2\n",
    "            smallest_masks = np.random.choice(smallest_masks, 2)\n",
    "            # remove masks larger than area 2000\n",
    "            smallest_masks = smallest_masks[mask_areas[smallest_masks] < 2000]\n",
    "            \n",
    "            \n",
    "            if np.random.rand() > 0.5:\n",
    "                for i in smallest_masks:\n",
    "                    mask = masks[i]\n",
    "                    \n",
    "                    # i)\n",
    "                    mask = np.roll(mask, np.random.randint(0, 512), axis=0)\n",
    "                    mask = np.roll(mask, np.random.randint(0, 512), axis=1)\n",
    "                    \n",
    "                    # ii)\n",
    "                    mask = np.flip(mask, axis=0)\n",
    "                    mask = np.flip(mask, axis=1)\n",
    "                    \n",
    "                    masks.append(mask)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            # get bounding box coordinates for each mask\n",
    "            num_objs = len(masks)\n",
    "            boxes = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        for i in range(num_objs):\n",
    "            pos = np.nonzero(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            #print(area,area.shape,area.dtype)\n",
    "        except:\n",
    "            area = torch.tensor([[0],[0]])\n",
    "        \n",
    "        \n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target['num_objs'] = torch.tensor(num_objs)      \n",
    "        target[\"image_id\"] = image_id  \n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "                \n",
    "        if self.mode == 'train':\n",
    "            \n",
    "            rand_width_height = 512 # np.random.randint(512, 1024)\n",
    "            # resize to 1024x1024\n",
    "            new_width = rand_width_height\n",
    "            new_height = rand_width_height\n",
    "            orig_height, orig_width = img.shape[1:]\n",
    "            # orig_height = img.size[1]\n",
    "            # orig_width = img.size[0]\n",
    "            \n",
    "            img = F.resize(img, [new_height, new_width], interpolation=InterpolationMode.BILINEAR)\n",
    "\n",
    "            if target is not None:\n",
    "                target[\"boxes\"][:, 0::2] *= new_width / orig_width\n",
    "                target[\"boxes\"][:, 1::2] *= new_height / orig_height\n",
    "                if \"masks\" in target:\n",
    "                    target[\"masks\"] = F.resize(\n",
    "                        target[\"masks\"], [new_height, new_width], interpolation=InterpolationMode.NEAREST\n",
    "                    )\n",
    "            \n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.246962Z",
     "iopub.status.busy": "2023-07-11T20:55:51.246818Z",
     "iopub.status.idle": "2023-07-11T20:55:51.248892Z",
     "shell.execute_reply": "2023-07-11T20:55:51.248702Z"
    }
   },
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomIoUCrop())\n",
    "        transforms.append(T.RandomZoomOut())\n",
    "        transforms.append(T.RandomPhotometricDistort())\n",
    "        transforms.append(T.ScaleJitter())\n",
    "        transforms.append(T.RandomShortestSize())\n",
    "        # transforms.append(T.FixedSizeCrop(size=(512,512)))\n",
    "        \n",
    "        \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.250040Z",
     "iopub.status.busy": "2023-07-11T20:55:51.249919Z",
     "iopub.status.idle": "2023-07-11T20:55:51.252379Z",
     "shell.execute_reply": "2023-07-11T20:55:51.252197Z"
    }
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.263931Z",
     "iopub.status.busy": "2023-07-11T20:55:51.263788Z",
     "iopub.status.idle": "2023-07-11T20:55:51.266884Z",
     "shell.execute_reply": "2023-07-11T20:55:51.266711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_imgs = len(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*'))\n",
    "n_imgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.268015Z",
     "iopub.status.busy": "2023-07-11T20:55:51.267921Z",
     "iopub.status.idle": "2023-07-11T20:55:51.269472Z",
     "shell.execute_reply": "2023-07-11T20:55:51.269301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/albumentations/augmentations/transforms.py:1554: UserWarning: Using default interpolation INTER_NEAREST, which is sub-optimal.Please specify interpolation mode for downscale and upscale explicitly.For additional information see this PR https://github.com/albumentations-team/albumentations/pull/584\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_imgs = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "all_masks = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "all_imgs = np.array(all_imgs)\n",
    "all_masks = np.array(all_masks)\n",
    "\n",
    "\n",
    "indices = [i for i in range(len(all_imgs))]\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=1234)\n",
    "\n",
    "train_img = all_imgs[train_indices]\n",
    "train_mask = all_masks[train_indices]\n",
    "val_img = all_imgs[val_indices]\n",
    "val_mask = all_masks[val_indices]\n",
    "dataset_train = PennFudanDataset(train_img, train_mask, mode='train')\n",
    "# dataset_val = PennFudanDataset(val_img, val_mask, get_transform(train=False), mode='val')\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=1, shuffle=True, num_workers=os.cpu_count(), pin_memory=True, drop_last=True, collate_fn=utils.collate_fn)\n",
    "# val_dl = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True,collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 512, 512]),\n",
       " torch.Size([3, 512, 512]),\n",
       " tensor(0.),\n",
       " tensor(0.9804))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, target = train_dl.dataset[0]\n",
    "# img.shape, target['masks']\n",
    "target['masks'].shape, img.shape, img.min(), img.max() # -> (torch.Size([8, 512, 512]), torch.Size([3, 512, 512]), tensor(0.), tensor(1.))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train dataset which is an augmentation of the original train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load /home/viktor/Documents/kaggle/hubmap-2023/experiments/yolo/yolov8-vanilla/YOLOv8x-maskrcnn-augms/datasets/train/images/0a4ddecc55f0.tif\n",
    "# img = tiff.imread('/home/viktor/Documents/kaggle/hubmap-2023/experiments/yolo/yolov8-vanilla/YOLOv8x-maskrcnn-augms/datasets/train/images/0a4ddecc55f0.tif')\n",
    "# img.min(), img.max(), img.shape # -> (0, 255, (512, 512, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = target['masks'][0]\n",
    "mask.shape # -> torch.Size([512, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming mask is a 2D torch tensor of shape [H, W]\n",
    "mask_np = mask.cpu().numpy() # Convert the mask to a numpy array\n",
    "\n",
    "# Find contours. Note, this will give the boundary pixels, \n",
    "# so polygons will be represented as a list of points along the boundary\n",
    "contours, _ = cv2.findContours(mask_np.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "polygons = [cnt.reshape(-1, 2) for cnt in contours] # Reshape for easier handling\n",
    "\n",
    "# Each element in polygons now represent a separate connected component in the original mask\n",
    "# polygons # [array([[121,  10], [120,  11], [119,  11],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_root = \"datasets/train-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming that torch_tensor is your image tensor\n",
    "torch_tensor = img\n",
    "\n",
    "# Denormalize the tensor from [0,1] to [0,255]\n",
    "torch_tensor_denorm = (torch_tensor * 255).byte()\n",
    "\n",
    "# Permute the tensor to make it suitable for creating a PIL image\n",
    "img_pil = Image.fromarray(torch_tensor_denorm.cpu().numpy().transpose(1,2,0))\n",
    "\n",
    "# Save the image\n",
    "img_pil.save('image.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1297 [00:00<?, ?it/s]/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      "/tmp/ipykernel_3590389/3052817082.py:183: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
      " 19%|█▉        | 248/1297 [00:44<03:06,  5.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m mask_np \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39m# Convert the mask to a numpy array\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Find contours. Note, this will give the boundary pixels, \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# so polygons will be represented as a list of points along the boundary\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m contours, _ \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mfindContours(mask_np\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39muint8\u001b[39;49m\u001b[39m'\u001b[39;49m), cv2\u001b[39m.\u001b[39;49mRETR_EXTERNAL, cv2\u001b[39m.\u001b[39;49mCHAIN_APPROX_SIMPLE)\n\u001b[1;32m     45\u001b[0m coordinates \u001b[39m=\u001b[39m [cnt\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m cnt \u001b[39min\u001b[39;00m contours] \u001b[39m# Reshape for easier handling\u001b[39;00m\n\u001b[1;32m     48\u001b[0m label_txt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0 \u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 16\n",
    "\n",
    "train_dl_indx = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for images, targets in tqdm(train_dl):\n",
    "        \n",
    "        img = images[0]\n",
    "        target = targets[0]\n",
    "        masks = target['masks']\n",
    "        \n",
    "        \n",
    "        # save img \n",
    "        # Assuming that torch_tensor is your image tensor\n",
    "        torch_tensor = img\n",
    "\n",
    "        # Denormalize the tensor from [0,1] to [0,255]\n",
    "        torch_tensor_denorm = (torch_tensor * 255).byte()\n",
    "\n",
    "        # Permute the tensor to make it suitable for creating a PIL image\n",
    "        img_pil = Image.fromarray(torch_tensor_denorm.cpu().numpy().transpose(1,2,0))\n",
    "\n",
    "        # Save the image\n",
    "        img_pil.save(f'datasets/train/images/{train_dl_indx}_3.tif')\n",
    "        \n",
    "\n",
    "        # save the mask\n",
    "        \n",
    "        label_txt = ''\n",
    "        \n",
    "        \n",
    "        for mask in masks:\n",
    "        \n",
    "        \n",
    "            # get polygons\n",
    "            # Assuming mask is a 2D torch tensor of shape [H, W]\n",
    "            mask_np = mask.cpu().numpy() # Convert the mask to a numpy array\n",
    "\n",
    "            # Find contours. Note, this will give the boundary pixels, \n",
    "            # so polygons will be represented as a list of points along the boundary\n",
    "            contours, _ = cv2.findContours(mask_np.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            coordinates = [cnt.reshape(-1, 2) for cnt in contours] # Reshape for easier handling\n",
    "            \n",
    "            \n",
    "            label_txt += '0 '\n",
    "            # Normalize\n",
    "            coor_array = np.array(coordinates[0]).astype(float)\n",
    "            coor_array /= float(512)\n",
    "            # transform to str\n",
    "            coor_list = list(coor_array.reshape(-1).astype(str))\n",
    "            coor_str = ' '.join(coor_list)\n",
    "            # add string to label txt\n",
    "            label_txt += f'{coor_str}\\n'\n",
    "        \n",
    "        # delete f'datasets/train/labels/{train_dl_indx}.txt'\n",
    "        if os.path.exists(f'datasets/train/labels/{train_dl_indx}_3.txt'):\n",
    "            os.remove(f'datasets/train/labels/{train_dl_indx}_3.txt')\n",
    "            \n",
    "        # Write labels to txt file\n",
    "        \n",
    "        with open(f'datasets/train/labels/{train_dl_indx}_3.txt', 'w') as f:\n",
    "            f.write(label_txt)\n",
    "        \n",
    "        train_dl_indx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
