{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.158863Z",
     "iopub.status.busy": "2023-07-11T20:55:50.158344Z",
     "iopub.status.idle": "2023-07-11T20:55:50.649519Z",
     "shell.execute_reply": "2023-07-11T20:55:50.649187Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.append(\"detection-wheel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.651038Z",
     "iopub.status.busy": "2023-07-11T20:55:50.650891Z",
     "iopub.status.idle": "2023-07-11T20:55:50.652551Z",
     "shell.execute_reply": "2023-07-11T20:55:50.652375Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.653765Z",
     "iopub.status.busy": "2023-07-11T20:55:50.653677Z",
     "iopub.status.idle": "2023-07-11T20:55:51.175336Z",
     "shell.execute_reply": "2023-07-11T20:55:51.175011Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs, masks, transforms, mode='train'):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = imgs#sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "        self.masks = masks#sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = self.imgs[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        #masks = (mask == obj_ids[:, None, None])\n",
    "        #print((obj_ids[:, None, None]).shape)\n",
    "        #masks = mask == obj_ids[:, None, None]\n",
    "        masks = [np.where(mask== obj_ids[i, None, None],1,0) for i in range(len(obj_ids))]\n",
    "        masks = np.array(masks)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.nonzero(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        try:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            #print(area,area.shape,area.dtype)\n",
    "        except:\n",
    "            area = torch.tensor([[0],[0]])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        #print(masks.shape)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                rand_width_height = 512 # np.random.randint(512, 1024)\n",
    "                # resize to 1024x1024\n",
    "                new_width = rand_width_height\n",
    "                new_height = rand_width_height\n",
    "                orig_height, orig_width = img.shape[1:]\n",
    "                # orig_height = img.size[1]\n",
    "                # orig_width = img.size[0]\n",
    "                \n",
    "                img = F.resize(img, [new_height, new_width], interpolation=InterpolationMode.BILINEAR)\n",
    "\n",
    "                if target is not None:\n",
    "                    target[\"boxes\"][:, 0::2] *= new_width / orig_width\n",
    "                    target[\"boxes\"][:, 1::2] *= new_height / orig_height\n",
    "                    if \"masks\" in target:\n",
    "                        target[\"masks\"] = F.resize(\n",
    "                            target[\"masks\"], [new_height, new_width], interpolation=InterpolationMode.NEAREST\n",
    "                        )\n",
    "\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.246962Z",
     "iopub.status.busy": "2023-07-11T20:55:51.246818Z",
     "iopub.status.idle": "2023-07-11T20:55:51.248892Z",
     "shell.execute_reply": "2023-07-11T20:55:51.248702Z"
    }
   },
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomIoUCrop())\n",
    "        transforms.append(T.RandomZoomOut())\n",
    "        transforms.append(T.RandomPhotometricDistort())\n",
    "        transforms.append(T.ScaleJitter())\n",
    "        transforms.append(T.RandomShortestSize())\n",
    "        # transforms.append(T.FixedSizeCrop(size=(512,512)))\n",
    "        \n",
    "        \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.250040Z",
     "iopub.status.busy": "2023-07-11T20:55:51.249919Z",
     "iopub.status.idle": "2023-07-11T20:55:51.252379Z",
     "shell.execute_reply": "2023-07-11T20:55:51.252197Z"
    }
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.263931Z",
     "iopub.status.busy": "2023-07-11T20:55:51.263788Z",
     "iopub.status.idle": "2023-07-11T20:55:51.266884Z",
     "shell.execute_reply": "2023-07-11T20:55:51.266711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_imgs = len(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*'))\n",
    "n_imgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.268015Z",
     "iopub.status.busy": "2023-07-11T20:55:51.267921Z",
     "iopub.status.idle": "2023-07-11T20:55:51.269472Z",
     "shell.execute_reply": "2023-07-11T20:55:51.269301Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_imgs = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "all_masks = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "all_imgs = np.array(all_imgs)\n",
    "all_masks = np.array(all_masks)\n",
    "\n",
    "\n",
    "indices = [i for i in range(len(all_imgs))]\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=1234)\n",
    "\n",
    "train_img = all_imgs[train_indices]\n",
    "train_mask = all_masks[train_indices]\n",
    "val_img = all_imgs[val_indices]\n",
    "val_mask = all_masks[val_indices]\n",
    "dataset_train = PennFudanDataset(train_img, train_mask, get_transform(train=True), mode='train')\n",
    "dataset_val = PennFudanDataset(val_img, val_mask, get_transform(train=False), mode='val')\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=1, shuffle=True, num_workers=os.cpu_count(), pin_memory=True, drop_last=True, collate_fn=utils.collate_fn)\n",
    "val_dl = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True,collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512, 512]),\n",
       " torch.Size([3, 512, 512]),\n",
       " tensor(0.0128),\n",
       " tensor(0.6728))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, target = train_dl.dataset[0]\n",
    "# img.shape, target['masks']\n",
    "target['masks'].shape, img.shape, img.min(), img.max() # -> (torch.Size([8, 512, 512]), torch.Size([3, 512, 512]), tensor(0.), tensor(1.))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train dataset which is an augmentation of the original train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load /home/viktor/Documents/kaggle/hubmap-2023/experiments/yolo/yolov8-vanilla/YOLOv8x-maskrcnn-augms/datasets/train/images/0a4ddecc55f0.tif\n",
    "# img = tiff.imread('/home/viktor/Documents/kaggle/hubmap-2023/experiments/yolo/yolov8-vanilla/YOLOv8x-maskrcnn-augms/datasets/train/images/0a4ddecc55f0.tif')\n",
    "# img.min(), img.max(), img.shape # -> (0, 255, (512, 512, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = target['masks'][0]\n",
    "mask.shape # -> torch.Size([512, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming mask is a 2D torch tensor of shape [H, W]\n",
    "mask_np = mask.cpu().numpy() # Convert the mask to a numpy array\n",
    "\n",
    "# Find contours. Note, this will give the boundary pixels, \n",
    "# so polygons will be represented as a list of points along the boundary\n",
    "contours, _ = cv2.findContours(mask_np.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "polygons = [cnt.reshape(-1, 2) for cnt in contours] # Reshape for easier handling\n",
    "\n",
    "# Each element in polygons now represent a separate connected component in the original mask\n",
    "# polygons # [array([[121,  10], [120,  11], [119,  11],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_root = \"datasets/train-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming that torch_tensor is your image tensor\n",
    "torch_tensor = img\n",
    "\n",
    "# Denormalize the tensor from [0,1] to [0,255]\n",
    "torch_tensor_denorm = (torch_tensor * 255).byte()\n",
    "\n",
    "# Permute the tensor to make it suitable for creating a PIL image\n",
    "img_pil = Image.fromarray(torch_tensor_denorm.cpu().numpy().transpose(1,2,0))\n",
    "\n",
    "# Save the image\n",
    "img_pil.save('image.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [09:26<00:00, 35.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 16\n",
    "\n",
    "train_dl_indx = 0\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for images, targets in train_dl:\n",
    "        \n",
    "        img = images[0]\n",
    "        target = targets[0]\n",
    "        masks = target['masks']\n",
    "        \n",
    "        \n",
    "        # save img \n",
    "        # Assuming that torch_tensor is your image tensor\n",
    "        torch_tensor = img\n",
    "\n",
    "        # Denormalize the tensor from [0,1] to [0,255]\n",
    "        torch_tensor_denorm = (torch_tensor * 255).byte()\n",
    "\n",
    "        # Permute the tensor to make it suitable for creating a PIL image\n",
    "        img_pil = Image.fromarray(torch_tensor_denorm.cpu().numpy().transpose(1,2,0))\n",
    "\n",
    "        # Save the image\n",
    "        img_pil.save(f'datasets/train/images/{train_dl_indx}.tif')\n",
    "        \n",
    "\n",
    "        # save the mask\n",
    "        \n",
    "        label_txt = ''\n",
    "        \n",
    "        \n",
    "        for mask in masks:\n",
    "        \n",
    "        \n",
    "            # get polygons\n",
    "            # Assuming mask is a 2D torch tensor of shape [H, W]\n",
    "            mask_np = mask.cpu().numpy() # Convert the mask to a numpy array\n",
    "\n",
    "            # Find contours. Note, this will give the boundary pixels, \n",
    "            # so polygons will be represented as a list of points along the boundary\n",
    "            contours, _ = cv2.findContours(mask_np.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            coordinates = [cnt.reshape(-1, 2) for cnt in contours] # Reshape for easier handling\n",
    "            \n",
    "            \n",
    "            label_txt += '0 '\n",
    "            # Normalize\n",
    "            coor_array = np.array(coordinates[0]).astype(float)\n",
    "            coor_array /= float(512)\n",
    "            # transform to str\n",
    "            coor_list = list(coor_array.reshape(-1).astype(str))\n",
    "            coor_str = ' '.join(coor_list)\n",
    "            # add string to label txt\n",
    "            label_txt += f'{coor_str}\\n'\n",
    "            \n",
    "        # Write labels to txt file\n",
    "        with open(f'datasets/train/labels/{train_dl_indx}.txt', 'w') as f:\n",
    "            f.write(label_txt)\n",
    "        \n",
    "        train_dl_indx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
