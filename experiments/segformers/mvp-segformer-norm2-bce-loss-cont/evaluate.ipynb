{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d674407",
   "metadata": {
    "papermill": {
     "duration": 0.012826,
     "end_time": "2023-06-25T08:17:09.270621",
     "exception": false,
     "start_time": "2023-06-25T08:17:09.257795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Although mAP metric is popular, I think we still need to write a new notebook to compute CV score\n",
    "1. Most of the mAP calculators are integrated within particular frameworks such as detectron2, yolo, mmdet, etc... It might lack flexibility if we want to ensemble various models.\n",
    "2. Some standalone codes only compute box-mAP, not segm-mAP\n",
    "\n",
    "=> So I reimplement the code to compute mAP for this competition. **Note that it only support single class at the moment**. I use my Detectron2 model to make predictions as an example and I also compare with Detectron2 built-in CocoEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2b63cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:17:23.531794Z",
     "iopub.status.busy": "2023-06-25T08:17:23.530172Z",
     "iopub.status.idle": "2023-06-25T08:17:27.830565Z",
     "shell.execute_reply": "2023-06-25T08:17:27.829481Z"
    },
    "papermill": {
     "duration": 4.313669,
     "end_time": "2023-06-25T08:17:27.833571",
     "exception": false,
     "start_time": "2023-06-25T08:17:23.519902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random, cv2, os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4412e6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:17:27.856356Z",
     "iopub.status.busy": "2023-06-25T08:17:27.855736Z",
     "iopub.status.idle": "2023-06-25T08:17:27.952289Z",
     "shell.execute_reply": "2023-06-25T08:17:27.951279Z"
    },
    "papermill": {
     "duration": 0.110578,
     "end_time": "2023-06-25T08:17:27.954790",
     "exception": false,
     "start_time": "2023-06-25T08:17:27.844212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import base64\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import zlib\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ab98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    MODEL_PATH = \"/home/viktor/Documents/kaggle/hubmap-2023/experiments/mvp-segformer-norm2-bce-loss/ckpts/segformer_epoch_83.pt\"\n",
    "    THRESHOLD = 0.37\n",
    "    IMG_DIR = \"/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/train\"\n",
    "    N_TRAIN = 1400 # Take first N_TRAIN images for training, rest for validation\n",
    "    \n",
    "    min_mask_area = 0 # minimum number of pixels of mask\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f04ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import json\n",
    "from PIL import Image\n",
    "from skimage.draw import polygon\n",
    "from albumentations import Compose, Resize, HorizontalFlip, VerticalFlip, BboxParams\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class HubmapDataset(Dataset):\n",
    "        \n",
    "    def __init__(self, image_dir, labels_file, n_train, mode='train'):\n",
    "        \n",
    "        assert mode in ['train', 'val'], \"mode must be one of ['train', 'val']\"\n",
    "        self.mode = mode\n",
    "        \n",
    "        with open(labels_file, 'r') as json_file:\n",
    "            self.json_labels = [json.loads(line) for line in json_file]\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.json_labels = self.json_labels[:n_train]\n",
    "        else:\n",
    "            self.json_labels = self.json_labels[n_train:]\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        \n",
    "        if mode == 'train':\n",
    "            initial_augm = [\n",
    "            \n",
    "                \n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                \n",
    "                A.RandomBrightnessContrast(p=0.5, brightness_limit=0.2, contrast_limit=0.2),\n",
    "                A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.1),\n",
    "                A.CoarseDropout(max_holes=1, max_width=int(512 * 0.05), max_height=int(512 * 0.05), \n",
    "                                mask_fill_value=0, p=0.1),\n",
    "                \n",
    "                \n",
    "                A.OneOf([\n",
    "                        A.GaussNoise(var_limit=[10, 50]),\n",
    "                        A.GaussianBlur(),\n",
    "                        A.MotionBlur(),\n",
    "                        ], p=0.1),\n",
    "                A.MultiplicativeNoise(per_channel=True, multiplier=(0.95, 1.05)),\n",
    "                \n",
    "            ]\n",
    "        else:\n",
    "            initial_augm = []\n",
    "        \n",
    "        self.aug_list = initial_augm + [\n",
    "                A.Resize(512, 512),\n",
    "                A.Normalize(\n",
    "                    mean= [0, 0, 0],\n",
    "                    std= [1, 1, 1],\n",
    "                    max_pixel_value = 255\n",
    "                ),\n",
    "                ToTensorV2(transpose_mask=True),\n",
    "            ]\n",
    "        \n",
    "        # Create the augmentation pipeline\n",
    "        self.augmentations = A.Compose(self.aug_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = os.path.join(self.image_dir, f\"{self.json_labels[idx]['id']}.tif\")\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        \n",
    "        # Get the mask\n",
    "        mask = np.zeros((512, 512), dtype=np.float32)\n",
    "        \n",
    "        for annot in self.json_labels[idx]['annotations']:\n",
    "            cords = annot['coordinates']\n",
    "            if annot['type'] == \"blood_vessel\":\n",
    "                for cord in cords:\n",
    "                    rr, cc = polygon(np.array([i[1] for i in cord]), np.asarray([i[0] for i in cord]))\n",
    "                    mask[rr, cc] = 1\n",
    "                    \n",
    "        image = np.array(image)\n",
    "\n",
    "        # image = torch.tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1)  # Shape: [C, H, W]\n",
    "        # mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        augmented = self.augmentations(image=image, mask=mask)\n",
    "        image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        \n",
    "        mean = torch.mean(image, dim=[1,2])\n",
    "        std = torch.std(image, dim=[1,2])\n",
    "        \n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "        \n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea390c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[39m=\u001b[39m HubmapDataset(image_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/train\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      2\u001b[0m                               labels_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/polygons.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      3\u001b[0m                               n_train\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mN_TRAIN,\n\u001b[1;32m      4\u001b[0m                               mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m val_dataset \u001b[39m=\u001b[39m HubmapDataset(image_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/train\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m                               labels_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/polygons.jsonl\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      8\u001b[0m                               n_train\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mN_TRAIN,\n\u001b[1;32m      9\u001b[0m                               mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mHubmapDataset.__init__\u001b[0;34m(self, image_dir, labels_file, n_train, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m mode\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(labels_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson_labels \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mloads(line) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m json_file]\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson_labels[:n_train]\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m mode\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(labels_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson_labels \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39;49mloads(line) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m json_file]\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson_labels[:n_train]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = HubmapDataset(image_dir=\"/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/train\", \n",
    "                              labels_file=\"/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/polygons.jsonl\", \n",
    "                              n_train=CFG.N_TRAIN,\n",
    "                              mode='train')\n",
    "\n",
    "val_dataset = HubmapDataset(image_dir=\"/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/train\", \n",
    "                              labels_file=\"/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/polygons.jsonl\", \n",
    "                              n_train=CFG.N_TRAIN,\n",
    "                              mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15538d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0e344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:17:28.019211Z",
     "iopub.status.busy": "2023-06-25T08:17:28.018881Z",
     "iopub.status.idle": "2023-06-25T08:17:35.830411Z",
     "shell.execute_reply": "2023-06-25T08:17:35.829402Z"
    },
    "papermill": {
     "duration": 7.824369,
     "end_time": "2023-06-25T08:17:35.833341",
     "exception": false,
     "start_time": "2023-06-25T08:17:28.008972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "import torch    \n",
    "import torch.nn as nn\n",
    "\n",
    "class Segformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seg_model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\",\n",
    "                                                         num_labels=1,\n",
    "                                                         ignore_mismatched_sizes=True,\n",
    "                                                         num_channels=3)\n",
    "        self.up = nn.Upsample(scale_factor=4, mode=\"bilinear\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.seg_model(x).logits\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = Segformer()\n",
    "\n",
    "\n",
    "state_dict = torch.load(CFG.MODEL_PATH)\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(CFG.device)\n",
    "\n",
    "\n",
    "model = model.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import segmentation_models_pytorch as smp\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"mit_b5\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,                      # model output channels (number of classes in your dataset)\n",
    "# )\n",
    "\n",
    "# state_dict = torch.load(CFG.MODEL_PATH)\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "# model = model.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4825486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def preprocess_img_for_model_inference(img_path):\n",
    "    # image = cv2.imread(img_path)\n",
    "    image = Image.open(img_path)\n",
    "    \n",
    "    image = np.array(image)\n",
    "    \n",
    "    \n",
    "    # apply \n",
    "    aug_list = [\n",
    "                A.Resize(512, 512),\n",
    "                A.Normalize(\n",
    "                    mean= [0, 0, 0],\n",
    "                    std= [1, 1, 1],\n",
    "                    max_pixel_value = 255\n",
    "                ),\n",
    "                ToTensorV2(transpose_mask=True),\n",
    "            ]\n",
    "        \n",
    "   # Create the augmentation pipeline\n",
    "    augmentations = A.Compose(aug_list)\n",
    "    \n",
    "    \n",
    "    # image = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    image = augmentations(image=image)[\"image\"].unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    mean = torch.mean(image, dim=[1,2])\n",
    "    std = torch.std(image, dim=[1,2])\n",
    "    \n",
    "    image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "    \n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_to_det(\n",
    "    seg: np.ndarray, \n",
    "):\n",
    "    num_outputs, labels, stats, centroids = cv2.connectedComponentsWithStats(seg)\n",
    "    boxes = stats[:, [cv2.CC_STAT_LEFT, cv2.CC_STAT_TOP, cv2.CC_STAT_WIDTH, cv2.CC_STAT_HEIGHT]]\n",
    "    label_masks = [labels == i for i in range(num_outputs)]\n",
    "    dets = {\n",
    "        \"boxes\": np.stack([\n",
    "            boxes[:, 0],\n",
    "            boxes[:, 1],\n",
    "            boxes[:, 0] + boxes[:, 2],\n",
    "            boxes[:, 1] + boxes[:, 3],\n",
    "        ], axis=1),\n",
    "        \"masks\": [seg * m for m in label_masks],\n",
    "    }\n",
    "    dets[\"scores\"] = [np.max(seg[m]) for m in label_masks]\n",
    "    \n",
    "    # remove dets element where 'boxes' = [0, 0, 512, 512]\n",
    "    boxes_to_remove = [0, 0, 512, 512]\n",
    "    indices_to_remove = np.where(np.all(dets[\"boxes\"] == boxes_to_remove, axis=1))\n",
    "    \n",
    "    dets[\"boxes\"] = np.delete(dets[\"boxes\"], indices_to_remove, axis=0)\n",
    "    dets[\"masks\"] = [i for j, i in enumerate(dets[\"masks\"]) if j not in indices_to_remove]\n",
    "    dets[\"scores\"] = np.delete(dets[\"scores\"], indices_to_remove)\n",
    "    \n",
    "    \n",
    "    # remove dets where np.sum(mask) < CFG.min_mask_area\n",
    "    indices_to_remove = []\n",
    "    for i, mask in enumerate(dets[\"masks\"]):\n",
    "        if np.sum(mask) < CFG.min_mask_area:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "    dets[\"boxes\"] = np.delete(dets[\"boxes\"], indices_to_remove, axis=0)\n",
    "    dets[\"masks\"] = [i for j, i in enumerate(dets[\"masks\"]) if j not in indices_to_remove]\n",
    "    dets[\"scores\"] = np.delete(dets[\"scores\"], indices_to_remove)\n",
    "    \n",
    "    return dets\n",
    "\n",
    "def predict(image):\n",
    "    with torch.no_grad():\n",
    "        pred = model(image)\n",
    "        pred = torch.sigmoid(pred)\n",
    "        pred = pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    pred = (pred > CFG.THRESHOLD).astype(np.uint8)*255\n",
    "    dets = seg_to_det(pred)\n",
    "    pred_masks, pred_classes, scores, boxes = dets[\"masks\"], [0]*len(dets[\"masks\"]), dets[\"scores\"], dets[\"boxes\"]\n",
    "    \n",
    "    # convert pred_masks to uint8_t\n",
    "    pred_masks = [m.astype(np.uint8) for m in pred_masks]\n",
    "    \n",
    "    return pred_masks, pred_classes, scores, boxes\n",
    "\n",
    "def predict_and_load_image(img_path):\n",
    "    image = preprocess_img_for_model_inference(img_path)\n",
    "    \n",
    "    \n",
    "    image = image.to(CFG.device)\n",
    "    return predict(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2330f2d",
   "metadata": {
    "papermill": {
     "duration": 0.020221,
     "end_time": "2023-06-25T08:17:44.794469",
     "exception": false,
     "start_time": "2023-06-25T08:17:44.774248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## segm-mAP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c66b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:17:44.837864Z",
     "iopub.status.busy": "2023-06-25T08:17:44.836994Z",
     "iopub.status.idle": "2023-06-25T08:17:44.841946Z",
     "shell.execute_reply": "2023-06-25T08:17:44.841091Z"
    },
    "papermill": {
     "duration": 0.029142,
     "end_time": "2023-06-25T08:17:44.844157",
     "exception": false,
     "start_time": "2023-06-25T08:17:44.815015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pycocotools.mask as mask_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a5f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:17:44.944853Z",
     "iopub.status.busy": "2023-06-25T08:17:44.944536Z",
     "iopub.status.idle": "2023-06-25T08:17:44.953187Z",
     "shell.execute_reply": "2023-06-25T08:17:44.952188Z"
    },
    "papermill": {
     "duration": 0.032517,
     "end_time": "2023-06-25T08:17:44.955174",
     "exception": false,
     "start_time": "2023-06-25T08:17:44.922657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def voc_ap(rec, prec):\n",
    "    \"\"\"\n",
    "    --- Official matlab code VOC2012---\n",
    "    mrec=[0 ; rec ; 1];\n",
    "    mpre=[0 ; prec ; 0];\n",
    "    for i=numel(mpre)-1:-1:1\n",
    "            mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    end\n",
    "    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    rec.append(1.0) # insert 1.0 at end of list\n",
    "    mrec = rec[:]\n",
    "    prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    prec.append(0.0) # insert 0.0 at end of list\n",
    "    mpre = prec[:]\n",
    "    \"\"\"\n",
    "     This part makes the precision monotonically decreasing\n",
    "        (goes from the end to the beginning)\n",
    "        matlab: for i=numel(mpre)-1:-1:1\n",
    "                    mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    \"\"\"\n",
    "    # matlab indexes start in 1 but python in 0, so I have to do:\n",
    "    #     range(start=(len(mpre) - 2), end=0, step=-1)\n",
    "    # also the python function range excludes the end, resulting in:\n",
    "    #     range(start=(len(mpre) - 2), end=-1, step=-1)\n",
    "    for i in range(len(mpre)-2, -1, -1):\n",
    "        mpre[i] = max(mpre[i], mpre[i+1])\n",
    "    \"\"\"\n",
    "     This part creates a list of indexes where the recall changes\n",
    "        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    \"\"\"\n",
    "    i_list = []\n",
    "    for i in range(1, len(mrec)):\n",
    "        if mrec[i] != mrec[i-1]:\n",
    "            i_list.append(i) # if it was matlab would be i + 1\n",
    "    \"\"\"\n",
    "     The Average Precision (AP) is the area under the curve\n",
    "        (numerical integration)\n",
    "        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    for i in i_list:\n",
    "        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
    "    return ap, mrec, mpre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbd15e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:17:44.997859Z",
     "iopub.status.busy": "2023-06-25T08:17:44.997573Z",
     "iopub.status.idle": "2023-06-25T08:17:45.014383Z",
     "shell.execute_reply": "2023-06-25T08:17:45.013152Z"
    },
    "papermill": {
     "duration": 0.041735,
     "end_time": "2023-06-25T08:17:45.016826",
     "exception": false,
     "start_time": "2023-06-25T08:17:44.975091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MAPCalculatorSingleClass:\n",
    "    def __init__(self, thresholds=[0.6]):\n",
    "        self.ious = []\n",
    "        self.confidences = []\n",
    "        self.image_ids = []\n",
    "        self.current_img_id = 0\n",
    "        \n",
    "        self.GT = 0\n",
    "        \n",
    "    def accumulate(self, ious, confidences, num_gt):\n",
    "        assert len(ious) == len(confidences), f\"ious and confidences must have same length: {len(ious)} != {len(confidences)}\"\n",
    "        self.ious.extend([x for x in ious])\n",
    "        self.confidences.extend(confidences)\n",
    "        self.image_ids.extend([self.current_img_id]*len(confidences))\n",
    "        self.current_img_id += 1\n",
    "        self.GT += num_gt\n",
    "        \n",
    "        \n",
    "    def evaluate(self, thresholds=[0.6], vis=False):        \n",
    "        # sort by confidence descending\n",
    "        sorted_inds = np.argsort(self.confidences)[::-1]\n",
    "        \n",
    "        results = dict()\n",
    "        \n",
    "        for th in thresholds:\n",
    "            accum_tp = 0\n",
    "            accum_fp = 0\n",
    "            \n",
    "            TP = []\n",
    "            FP = []\n",
    "            list_gts = dict()\n",
    "            \n",
    "            NPREDS = []\n",
    "            count = 0\n",
    "            for ind in sorted_inds:\n",
    "                iou_row = self.ious[ind]\n",
    "                img_id = self.image_ids[ind]\n",
    "                \n",
    "                matched_inds = np.where(iou_row >= th)[0]\n",
    "                best_gt_ind = -1\n",
    "                best_iou = 0\n",
    "                for gt_ind in matched_inds:\n",
    "                    iou = iou_row[gt_ind]\n",
    "                    if iou > best_iou and list_gts.get((img_id, gt_ind)) is None:\n",
    "                        best_iou = iou\n",
    "                        best_gt_ind = gt_ind\n",
    "                \n",
    "                if best_gt_ind != -1:\n",
    "                    list_gts[(img_id, best_gt_ind)] = True\n",
    "                    accum_tp += 1\n",
    "                else:\n",
    "                    accum_fp += 1\n",
    "                    \n",
    "                count += 1\n",
    "                NPREDS.append(count)\n",
    "                    \n",
    "                TP.append(accum_tp)\n",
    "                FP.append(accum_fp)\n",
    "                \n",
    "            PR = []\n",
    "            REC = []\n",
    "            \n",
    "            for tp, fp in zip(TP, FP):\n",
    "                pr = tp / (tp+fp+1e-5)\n",
    "                rec = tp / self.GT\n",
    "                PR.append(pr)\n",
    "                REC.append(rec)\n",
    "                \n",
    "            if vis:\n",
    "                plt.figure()\n",
    "                plt.plot(REC, PR, '-o', label='precision-recall curve')\n",
    "                \n",
    "            ap, mrec, mpre = voc_ap(REC, PR)\n",
    "            \n",
    "            if vis:\n",
    "                plt.plot(mrec, mpre, '--', label='interpolation')\n",
    "                plt.legend()\n",
    "                plt.title('Precision recall curve at threshold:'+str(np.round(th, 2)))\n",
    "                plt.show()\n",
    "            \n",
    "            results[th] = ap\n",
    "    \n",
    "        outputs = {}\n",
    "        outputs[\"true_positives\"] = TP\n",
    "        outputs[\"false_positives\"] = FP\n",
    "        outputs[\"precision\"] = PR\n",
    "        outputs[\"recall\"] = REC\n",
    "    \n",
    "        return np.mean(list(results.values())), results, outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9364a5b",
   "metadata": {
    "papermill": {
     "duration": 0.020518,
     "end_time": "2023-06-25T08:17:45.488345",
     "exception": false,
     "start_time": "2023-06-25T08:17:45.467827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Our custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9ea20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:17:45.531793Z",
     "iopub.status.busy": "2023-06-25T08:17:45.531034Z",
     "iopub.status.idle": "2023-06-25T08:18:16.780045Z",
     "shell.execute_reply": "2023-06-25T08:18:16.779066Z"
    },
    "papermill": {
     "duration": 31.273777,
     "end_time": "2023-06-25T08:18:16.782626",
     "exception": false,
     "start_time": "2023-06-25T08:17:45.508849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.morphology import binary_dilation\n",
    "mAP_calc = MAPCalculatorSingleClass()\n",
    "\n",
    "pbar = enumerate(val_dataset)\n",
    "pbar = tqdm(pbar, total=len(val_dataset))\n",
    "    \n",
    "areas_gt = []\n",
    "areas_pred = []\n",
    "\n",
    "heights_gt = []\n",
    "heights_pred = []\n",
    "\n",
    "widths_gt = []\n",
    "widths_pred = []\n",
    "\n",
    "aspect_ratios_gt = []\n",
    "aspect_ratios_pred = []\n",
    "\n",
    "true_positives_total = 0\n",
    "false_positives_total = 0\n",
    "\n",
    "for i, (images, target_masks) in pbar:\n",
    "    \n",
    "    \n",
    "    width, height = images.shape[2], images.shape[1]\n",
    "    images = images.unsqueeze(0).to(CFG.device)    \n",
    "    \n",
    "    dets = seg_to_det(target_masks.squeeze(0).cpu().numpy().astype(np.uint8)*255)\n",
    "    \n",
    "    target_masks_all = target_masks.squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    target_masks, pred_classes, scores_target, bboxes_target = dets[\"masks\"], [0]*len(dets[\"masks\"]), dets[\"scores\"], dets[\"boxes\"]\n",
    "    # convert pred_masks to uint8_t\n",
    "    target_masks = [m.astype(np.uint8) for m in target_masks]\n",
    "\n",
    "    # make prediction\n",
    "    # pred_masks, pred_classes, scores = predict(predictor, img)\n",
    "    pred_masks, pred_classes, scores_pred, bboxes_pred = predict(images)\n",
    "    \n",
    "    pred_masks_all = np.sum(pred_masks, axis=0)\n",
    "    \n",
    "    \n",
    "    # calculate number of true positives\n",
    "    true_positives = np.sum(pred_masks_all * target_masks_all)\n",
    "    false_positives = np.sum(pred_masks_all * (1 - target_masks_all))\n",
    "    \n",
    "    true_positives_total += true_positives\n",
    "    false_positives_total += false_positives\n",
    "    \n",
    "    min_area_dilation = 1e4\n",
    "    # dilate all pred masks below min_area_dilation\n",
    "    # for i, m in enumerate(pred_masks):\n",
    "    #     if m.sum() < min_area_dilation:\n",
    "    #         pred_masks[i] = binary_dilation(m/255).astype(np.uint8)*255\n",
    "        \n",
    "    \n",
    "    min_area = 0e4\n",
    "    max_area = 5e4\n",
    "    min_height = 5\n",
    "    min_width = 5\n",
    "    # # remove pred_masks with area < 200\n",
    "    remove_inds = []\n",
    "    for i, m in enumerate(pred_masks):\n",
    "        if (m.sum() < max_area and m.sum() > min_area) or (bboxes_pred[i,3] - bboxes_pred[i,1] < min_height or bboxes_pred[i,2] - bboxes_pred[i,0] < min_width):\n",
    "            remove_inds.append(i)\n",
    "    pred_masks = [m for i, m in enumerate(pred_masks) if i not in remove_inds]\n",
    "    pred_classes = [c for i, c in enumerate(pred_classes) if i not in remove_inds]\n",
    "    scores_pred = [s for i, s in enumerate(scores_pred) if i not in remove_inds]\n",
    "    bboxes_pred = bboxes_pred[~np.isin(np.arange(len(bboxes_pred)), remove_inds)]\n",
    "    \n",
    "    # remove_inds = []\n",
    "    # for i, m in enumerate(target_masks):\n",
    "    #     if m.sum() < min_area or (bboxes_target[i,3] - bboxes_target[i,1] < min_height and bboxes_target[i,2] - bboxes_target[i,0] < min_width):\n",
    "    #         remove_inds.append(i)\n",
    "    # target_masks = [m for i, m in enumerate(target_masks) if i not in remove_inds]\n",
    "    # pred_classes = [c for i, c in enumerate(pred_classes) if i not in remove_inds]\n",
    "    # scores_target = [s for i, s in enumerate(scores_target) if i not in remove_inds]\n",
    "    # bboxes_target = bboxes_target[~np.isin(np.arange(len(bboxes_target)), remove_inds)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i, m in enumerate(pred_masks):\n",
    "        areas_pred.append(m.sum())\n",
    "        \n",
    "        height = bboxes_pred[i, 2] - bboxes_pred[i, 0]\n",
    "        heights_pred.append(height)\n",
    "        \n",
    "        width = bboxes_pred[i, 3] - bboxes_pred[i, 1]\n",
    "        widths_pred.append(width)\n",
    "        \n",
    "        aspect_ratio = width / height\n",
    "        aspect_ratios_pred.append(aspect_ratio)\n",
    "        \n",
    "    \n",
    "    for i, m in enumerate(target_masks):\n",
    "        areas_gt.append(m.sum())\n",
    "        \n",
    "        height = bboxes_target[i, 2] - bboxes_target[i, 0]\n",
    "        heights_gt.append(height)\n",
    "        \n",
    "        width = bboxes_target[i, 3] - bboxes_target[i, 1]\n",
    "        widths_gt.append(width)\n",
    "        \n",
    "        aspect_ratio = width / height\n",
    "        aspect_ratios_gt.append(aspect_ratio)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # pred_masks: list of numpy array shape H*W \n",
    "    enc_preds = [mask_util.encode(np.asarray(p, order='F')) for p in pred_masks]\n",
    "    enc_targs = [mask_util.encode(np.asarray(p, order='F')) for p in target_masks]\n",
    "    \n",
    "    num_gts = len(enc_targs)\n",
    "    \n",
    "    # calculate iou\n",
    "    if len(enc_targs) > 0:\n",
    "        ious = mask_util.iou(enc_preds, enc_targs, [0]*len(enc_targs))\n",
    "    else:\n",
    "        ious = np.array([[0]]*len(enc_preds))\n",
    "    \n",
    "    \n",
    "    # acummulate predictions\n",
    "    if num_gts > 0:\n",
    "        mAP_calc.accumulate(ious, scores_pred, num_gts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e244437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:18:16.826330Z",
     "iopub.status.busy": "2023-06-25T08:18:16.825666Z",
     "iopub.status.idle": "2023-06-25T08:18:17.957852Z",
     "shell.execute_reply": "2023-06-25T08:18:17.956931Z"
    },
    "papermill": {
     "duration": 1.156568,
     "end_time": "2023-06-25T08:18:17.960458",
     "exception": false,
     "start_time": "2023-06-25T08:18:16.803890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mAP, detail_scores, outputs = mAP_calc.evaluate(thresholds=np.arange(0.5, 1.0, 0.05))\n",
    "print('segm-mAP@0.5:0.95 by custom code:', mAP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "972903b2",
   "metadata": {
    "papermill": {
     "duration": 0.021451,
     "end_time": "2023-06-25T08:18:52.437800",
     "exception": false,
     "start_time": "2023-06-25T08:18:52.416349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Competition metric\n",
    "- In this competition, the metric is segm-mAP@0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1f99c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:18:52.482594Z",
     "iopub.status.busy": "2023-06-25T08:18:52.481563Z",
     "iopub.status.idle": "2023-06-25T08:18:52.939056Z",
     "shell.execute_reply": "2023-06-25T08:18:52.938094Z"
    },
    "papermill": {
     "duration": 0.483142,
     "end_time": "2023-06-25T08:18:52.942063",
     "exception": false,
     "start_time": "2023-06-25T08:18:52.458921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We test on the above dataset and get the CV score and visualize precision-recall curve\n",
    "mAP, detail_scores, outputs = mAP_calc.evaluate(thresholds=[0.6], vis=True)\n",
    "print('CV (mAP@0.6):', mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = true_positives_total / (true_positives_total + false_positives_total)\n",
    "acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1845125c",
   "metadata": {},
   "source": [
    "threshold,accuracy\n",
    "\n",
    "- 0.1  -> 0.58\n",
    "- 0.37 -> 0.727\n",
    "- 0.5  -> 0.765\n",
    "- 0.6  -> 0.789\n",
    "- 0.9  -> 0.855\n",
    "- 0.95  -> 0.869\n",
    "- 0.99  -> 0.877"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3a09a3a",
   "metadata": {},
   "source": [
    "# dilation study (no excluding of small areas)\n",
    "\n",
    "\n",
    "min area: 0 (no dilation) -> CV (mAP@0.6): 0.29340945900722193\n",
    "\n",
    "\n",
    "min area: 0 (dilation of <1e4 area) -> CV (mAP@0.6): 0.29340945900722193\n",
    "\n",
    "\n",
    "min area: 0 (dilation of <6e4 area) -> CV (mAP@0.6): 0.3025598833942809\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dilation study (no excluding of small areas)\n",
    "\n",
    "\n",
    "min area: 0 (no dilation) -> CV (mAP@0.6): 0.29340945900722193\n",
    "\n",
    "\n",
    "min area: 0 (dilation of <1e4 area + excluding areas between 0 and 2e4) -> CV (mAP@0.6): 0.33750069599634336\n",
    "\n",
    "\n",
    "min area: 0 (dilation of <1e4 area + excluding areas between 0 and 5e4) -> CV (mAP@0.6): 0.3523174522299635\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f4e55f8",
   "metadata": {},
   "source": [
    "min area: 0 -> CV (mAP@0.6): 0.3139922130574431\n",
    "\n",
    "\n",
    "min area: 2e4 -> CV (mAP@0.6): 0.33750069599634336\n",
    "\n",
    "\n",
    "min area: 4e4 -> CV (mAP@0.6): 0.3515683621145983\n",
    "\n",
    "\n",
    "min area: 5e4 -> CV (mAP@0.6): 0.3523174\n",
    "\n",
    "\n",
    "min area: 6e4 -> CV (mAP@0.6): 0.3474716751175245\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011361c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85baa785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4860475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493290a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(areas_gt, bins=1000, label='gt', alpha=0.5);\n",
    "plt.hist(areas_pred, bins=1000, label='pred', alpha=0.5);\n",
    "\n",
    "# set vertical line at 64*64\n",
    "plt.axvline(255*8*8, linestyle='dashed', linewidth=1, label='8*8')\n",
    "plt.axvline(255*16*16, linestyle='dashed', linewidth=1, label='16*16')\n",
    "plt.axvline(255*32*32, linestyle='dashed', linewidth=1, label='32*32')\n",
    "plt.axvline(255*64*64, linestyle='dashed', linewidth=1, label='64*64')\n",
    "plt.axvline(255*128*128, linestyle='dashed', linewidth=1, label='128*128')\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(heights_gt, bins=100, label='gt', alpha=0.5);\n",
    "plt.hist(heights_pred, bins=100, label='pred', alpha=0.5);\n",
    "\n",
    "plt.xlim(0, 100)\n",
    "plt.legend()\n",
    "plt.xlabel('height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(widths_gt, bins=100, label='gt', alpha=0.5);\n",
    "plt.hist(widths_pred, bins=100, label='pred', alpha=0.5);\n",
    "\n",
    "plt.xlim(0, 100)\n",
    "plt.legend()\n",
    "plt.xlabel('width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe166dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan from aspect_ratios_gt\n",
    "aspect_ratios_gt = [a for a in aspect_ratios_gt if not np.isnan(a) and not np.isinf(a) and np.abs(a) < 5]\n",
    "aspect_ratios_pred  = [a for a in aspect_ratios_pred if not np.isnan(a) and not np.isinf(a) and np.abs(a) < 5]\n",
    "\n",
    "plt.hist(aspect_ratios_gt, bins=100, label='gt', alpha=0.5);\n",
    "plt.hist(aspect_ratios_pred, bins=100, label='pred', alpha=0.5);\n",
    "\n",
    "# plt.xlim(0, 2)\n",
    "plt.legend()\n",
    "plt.xlabel('aspect_ratio = width / height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d276c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of width and height\n",
    "plt.scatter(widths_gt, heights_gt, label='gt', alpha=0.5);\n",
    "plt.scatter(widths_pred, heights_pred, label='pred', alpha=0.5);\n",
    "\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of width and height\n",
    "plt.scatter(widths_gt, heights_gt, label='gt', alpha=0.5)\n",
    "plt.scatter(widths_pred, heights_pred, label='pred', alpha=0.5)\n",
    "\n",
    "# xlim 0 to 100\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52d793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a16fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_gt = np.sort(areas_gt)\n",
    "indices = np.arange(0, len(areas_gt), 1)\n",
    "indices_cumsum = np.cumsum(indices)\n",
    "plt.plot(areas_gt, indices_cumsum, label='gt')\n",
    "\n",
    "\n",
    "areas_pred = np.sort(areas_pred)\n",
    "indices = np.arange(0, len(areas_pred), 1)\n",
    "indices_cumsum = np.cumsum(indices)\n",
    "\n",
    "plt.plot(areas_pred, indices_cumsum, label='pred')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_gt = np.sort(areas_gt)\n",
    "indices = np.arange(0, len(areas_gt), 1)\n",
    "indices_cumsum = np.cumsum(indices)\n",
    "# get derivative of indices_cumsum with respect to areas_gt\n",
    "indices_cumsum_diff = np.diff(indices_cumsum) / np.diff(areas_gt)\n",
    "plt.plot(areas_gt[:-1], indices_cumsum_diff, label='gt')\n",
    "\n",
    "\n",
    "areas_pred = np.sort(areas_pred)\n",
    "indices = np.arange(0, len(areas_pred), 1)\n",
    "indices_cumsum = np.cumsum(indices)\n",
    "indices_cumsum_diff = np.diff(indices_cumsum) / np.diff(areas_pred)\n",
    "plt.plot(areas_pred[:-1], indices_cumsum_diff, label='pred')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0, 0.2e7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 118.831912,
   "end_time": "2023-06-25T08:18:56.920762",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-25T08:16:58.088850",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09157387cf7642ea8a91f1251bd64878": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "176fa2117bc643e6bc01855fbb60fa83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1990e26737ed4412a168feceab89a76b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_23412cb71c7d466083fa36439486c7be",
        "IPY_MODEL_1dec2077b9d5485895eb1435f2019414",
        "IPY_MODEL_a5166c22758f4181b611aa4fac453449"
       ],
       "layout": "IPY_MODEL_44f935b54893456e9b311aba26dc3c74"
      }
     },
     "1dec2077b9d5485895eb1435f2019414": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_176fa2117bc643e6bc01855fbb60fa83",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_09157387cf7642ea8a91f1251bd64878",
       "value": 1
      }
     },
     "23412cb71c7d466083fa36439486c7be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d2376cdc116a4a5cb414c4928a4d2f39",
       "placeholder": "",
       "style": "IPY_MODEL_97a90398452d4db39f388e1e1bce539e",
       "value": "100%"
      }
     },
     "23a41155e136489db06eeb75d9d511cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a0af7320462d4f7390013dac19f06c40",
       "placeholder": "",
       "style": "IPY_MODEL_732a0fb5913047f6af233f5fee6cd923",
       "value": " 293/293 [00:31&lt;00:00,  9.54it/s]"
      }
     },
     "44f935b54893456e9b311aba26dc3c74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "67b4f53be0184f37a82b9eeb3d4cfbf9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "732a0fb5913047f6af233f5fee6cd923": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8585f974c47841b9908ba239517e986f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "87f9adde3dff4a51914b1633391298c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ea4a1c3dbf1246d09aafb91179c09bd1",
       "max": 293,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8585f974c47841b9908ba239517e986f",
       "value": 293
      }
     },
     "97a90398452d4db39f388e1e1bce539e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a0af7320462d4f7390013dac19f06c40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a3089a91e23c43e6a5983a62455f6fbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ac4fc6de8a9f415baaaffd6a2eb18787",
       "placeholder": "",
       "style": "IPY_MODEL_67b4f53be0184f37a82b9eeb3d4cfbf9",
       "value": "100%"
      }
     },
     "a5166c22758f4181b611aa4fac453449": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_efec3941c4af4ebb9ad514ec6cfa351e",
       "placeholder": "",
       "style": "IPY_MODEL_e512973f5d4b44aa910a55ff0f131195",
       "value": " 1/1 [00:00&lt;00:00,  8.70it/s]"
      }
     },
     "ac4fc6de8a9f415baaaffd6a2eb18787": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7705fc34b284aa38bd8c1ac62d4344c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a3089a91e23c43e6a5983a62455f6fbe",
        "IPY_MODEL_87f9adde3dff4a51914b1633391298c7",
        "IPY_MODEL_23a41155e136489db06eeb75d9d511cd"
       ],
       "layout": "IPY_MODEL_b7cadf7a16ad44ad8093eb2fb8ae2fd0"
      }
     },
     "b7cadf7a16ad44ad8093eb2fb8ae2fd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2376cdc116a4a5cb414c4928a4d2f39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e512973f5d4b44aa910a55ff0f131195": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ea4a1c3dbf1246d09aafb91179c09bd1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efec3941c4af4ebb9ad514ec6cfa351e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
