{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.158863Z",
     "iopub.status.busy": "2023-07-11T20:55:50.158344Z",
     "iopub.status.idle": "2023-07-11T20:55:50.649519Z",
     "shell.execute_reply": "2023-07-11T20:55:50.649187Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.append(\"detection-wheel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.651038Z",
     "iopub.status.busy": "2023-07-11T20:55:50.650891Z",
     "iopub.status.idle": "2023-07-11T20:55:50.652551Z",
     "shell.execute_reply": "2023-07-11T20:55:50.652375Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.653765Z",
     "iopub.status.busy": "2023-07-11T20:55:50.653677Z",
     "iopub.status.idle": "2023-07-11T20:55:51.175336Z",
     "shell.execute_reply": "2023-07-11T20:55:51.175011Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs, masks, transforms):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = imgs#sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "        self.masks = masks#sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = self.imgs[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        #masks = (mask == obj_ids[:, None, None])\n",
    "        #print((obj_ids[:, None, None]).shape)\n",
    "        #masks = mask == obj_ids[:, None, None]\n",
    "        masks = [np.where(mask== obj_ids[i, None, None],1,0) for i in range(len(obj_ids))]\n",
    "        masks = np.array(masks)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.nonzero(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        try:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            #print(area,area.shape,area.dtype)\n",
    "        except:\n",
    "            area = torch.tensor([[0],[0]])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        #print(masks.shape)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.176860Z",
     "iopub.status.busy": "2023-07-11T20:55:51.176708Z",
     "iopub.status.idle": "2023-07-11T20:55:51.228454Z",
     "shell.execute_reply": "2023-07-11T20:55:51.228169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasterrcnn_mobilenet_v3_large_320_fpn',\n",
       " 'fasterrcnn_mobilenet_v3_large_fpn',\n",
       " 'fasterrcnn_resnet50_fpn',\n",
       " 'fasterrcnn_resnet50_fpn_v2',\n",
       " 'fcos_resnet50_fpn',\n",
       " 'keypointrcnn_resnet50_fpn',\n",
       " 'maskrcnn_resnet50_fpn',\n",
       " 'maskrcnn_resnet50_fpn_v2',\n",
       " 'retinanet_resnet50_fpn',\n",
       " 'retinanet_resnet50_fpn_v2',\n",
       " 'ssd300_vgg16',\n",
       " 'ssdlite320_mobilenet_v3_large']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models import list_models\n",
    "detection_models = list_models(module=torchvision.models.detection)\n",
    "detection_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.243653Z",
     "iopub.status.busy": "2023-07-11T20:55:51.243517Z",
     "iopub.status.idle": "2023-07-11T20:55:51.245790Z",
     "shell.execute_reply": "2023-07-11T20:55:51.245599Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models import resnet101\n",
    "from torch import nn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # Load a pre-trained resnet101 model for classification and return only the features\n",
    "    backbone = resnet101(pretrained=True)\n",
    "    backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "\n",
    "    # MaskRCNN needs to know the number of output channels in a backbone. \n",
    "    # For resnet101, it's 2048 so we need to add it here\n",
    "    backbone.out_channels = 2048\n",
    "\n",
    "    # let's make the RPN generate 5 x 3 anchors per spatial\n",
    "    # location, with 5 different sizes and 3 different aspect\n",
    "    # ratios.\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "    # define what are the feature maps that we will use to perform the region of interest cropping\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "\n",
    "    # put the pieces together inside a FasterRCNN model\n",
    "    model = MaskRCNN(backbone,\n",
    "                    num_classes=num_classes,\n",
    "                    rpn_anchor_generator=anchor_generator,\n",
    "                    box_roi_pool=roi_pooler)\n",
    "    \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet101(pretrained=True).fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roi_heads.box_predictor.cls_score.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNNPredictor(\n",
       "  (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\", weights_backbone=ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.roi_heads.mask_predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNNPredictor(\n",
       "  (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roi_heads.mask_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNNPredictor(\n",
       "  (conv5_mask): ConvTranspose2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (mask_fcn_logits): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaskRCNNPredictor(256,\n",
    "                        512,\n",
    "                        2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.246962Z",
     "iopub.status.busy": "2023-07-11T20:55:51.246818Z",
     "iopub.status.idle": "2023-07-11T20:55:51.248892Z",
     "shell.execute_reply": "2023-07-11T20:55:51.248702Z"
    }
   },
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomIoUCrop())\n",
    "        transforms.append(T.RandomZoomOut())\n",
    "        transforms.append(T.RandomPhotometricDistort())\n",
    "        transforms.append(T.ScaleJitter())\n",
    "        transforms.append(T.RandomShortestSize())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.250040Z",
     "iopub.status.busy": "2023-07-11T20:55:51.249919Z",
     "iopub.status.idle": "2023-07-11T20:55:51.252379Z",
     "shell.execute_reply": "2023-07-11T20:55:51.252197Z"
    }
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.253503Z",
     "iopub.status.busy": "2023-07-11T20:55:51.253387Z",
     "iopub.status.idle": "2023-07-11T20:55:51.262727Z",
     "shell.execute_reply": "2023-07-11T20:55:51.262523Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.263931Z",
     "iopub.status.busy": "2023-07-11T20:55:51.263788Z",
     "iopub.status.idle": "2023-07-11T20:55:51.266884Z",
     "shell.execute_reply": "2023-07-11T20:55:51.266711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_imgs = len(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*'))\n",
    "n_imgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.268015Z",
     "iopub.status.busy": "2023-07-11T20:55:51.267921Z",
     "iopub.status.idle": "2023-07-11T20:55:51.269472Z",
     "shell.execute_reply": "2023-07-11T20:55:51.269301Z"
    }
   },
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "# for i, (train_index, test_index) in enumerate(kf.split(range(n_imgs))):\n",
    "#     if i!=0: continue\n",
    "#     all_imgs = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "#     all_masks = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "#     all_imgs = np.array(all_imgs)\n",
    "#     all_masks = np.array(all_masks)\n",
    "#     train_img = all_imgs[train_index]\n",
    "#     train_mask = all_masks[train_index]\n",
    "#     val_img = all_imgs[test_index]\n",
    "#     val_mask = all_masks[test_index]\n",
    "#     dataset_train = PennFudanDataset(train_img, train_mask, get_transform(train=True))\n",
    "#     dataset_val = PennFudanDataset(val_img, val_mask, get_transform(train=False))\n",
    "#     train_dl = torch.utils.data.DataLoader(\n",
    "#         dataset_train, batch_size=4, shuffle=True, num_workers=os.cpu_count(), pin_memory=True, drop_last=True, collate_fn=utils.collate_fn)\n",
    "#     val_dl = torch.utils.data.DataLoader(\n",
    "#         dataset_val, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True,collate_fn=utils.collate_fn)\n",
    "    \n",
    "#     model = get_model_instance_segmentation(num_classes=2)\n",
    "#     model.to(device)\n",
    "#     params = [p for p in model.parameters() if p.requires_grad]\n",
    "#     optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-6)\n",
    "#     # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "#     # set linear warmup scheduler, with constant learning rate after warmup\n",
    "#     scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0001,\n",
    "#                                                 steps_per_epoch=10, epochs=EPOCHS//10,\n",
    "#                                                 pct_start=0.01)\n",
    "    \n",
    "#     for epoch in range(EPOCHS):\n",
    "#         train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq=50)\n",
    "#         evaluate(model, val_dl, device=device)\n",
    "#         scheduler.step()\n",
    "#         model_path = f'fold_{i}_epoch{epoch}.pth'\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.270596Z",
     "iopub.status.busy": "2023-07-11T20:55:51.270487Z",
     "iopub.status.idle": "2023-07-11T20:55:51.272248Z",
     "shell.execute_reply": "2023-07-11T20:55:51.272071Z"
    }
   },
   "outputs": [],
   "source": [
    "all_indices = np.arange(n_imgs)\n",
    "# take random 1400 images for training\n",
    "train_index = np.random.choice(all_indices, size=1400, replace=False)\n",
    "# take the rest for validation\n",
    "test_index = np.setdiff1d(all_indices, train_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.273372Z",
     "iopub.status.busy": "2023-07-11T20:55:51.273257Z",
     "iopub.status.idle": "2023-07-11T20:55:51.274962Z",
     "shell.execute_reply": "2023-07-11T20:55:51.274789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if train_index and test_index are mutually exclusive\n",
    "len(np.intersect1d(train_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Create a custom function to log output\n",
    "# def log_output(text):\n",
    "#     with open('output.log', 'a') as f:\n",
    "#         f.write(text)\n",
    "\n",
    "# # Redirect stdout to the custom log function\n",
    "# sys.stdout.write = log_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.275939Z",
     "iopub.status.busy": "2023-07-11T20:55:51.275834Z",
     "iopub.status.idle": "2023-07-12T00:21:56.837704Z",
     "shell.execute_reply": "2023-07-12T00:21:56.837259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mLinearLR(optimizer,start_factor\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[1;32m     24\u001b[0m                                             end_factor\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     25\u001b[0m                                             total_iters\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 28\u001b[0m     train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m     evaluate(model, val_dl, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     30\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/vanilla-mask-rcnn-2-resnet-101/detection-wheel/engine.py:31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     29\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 31\u001b[0m     loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     32\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     34\u001b[0m \u001b[39m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/models/detection/generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[1;32m    104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m--> 105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/models/detection/roi_heads.py:761\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    758\u001b[0m     regression_targets \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     matched_idxs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m box_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbox_roi_pool(features, proposals, image_shapes)\n\u001b[1;32m    762\u001b[0m box_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_head(box_features)\n\u001b[1;32m    763\u001b[0m class_logits, box_regression \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_predictor(box_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/ops/poolers.py:310\u001b[0m, in \u001b[0;36mMultiScaleRoIAlign.forward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    308\u001b[0m x_filtered \u001b[39m=\u001b[39m _filter_input(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatmap_names)\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscales \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_levels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscales, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_levels \u001b[39m=\u001b[39m _setup_scales(\n\u001b[1;32m    311\u001b[0m         x_filtered, image_shapes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcanonical_scale, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcanonical_level\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m _multiscale_roi_align(\n\u001b[1;32m    315\u001b[0m     x_filtered,\n\u001b[1;32m    316\u001b[0m     boxes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_levels,\n\u001b[1;32m    321\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env/lib/python3.9/site-packages/torchvision/ops/poolers.py:125\u001b[0m, in \u001b[0;36m_setup_scales\u001b[0;34m(features, image_shapes, canonical_scale, canonical_level)\u001b[0m\n\u001b[1;32m    122\u001b[0m scales \u001b[39m=\u001b[39m [_infer_scale(feat, original_input_shape) \u001b[39mfor\u001b[39;00m feat \u001b[39min\u001b[39;00m features]\n\u001b[1;32m    123\u001b[0m \u001b[39m# get the levels in the feature map by leveraging the fact that the network always\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# downsamples by a factor of 2 at each level.\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m lvl_min \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog2(torch\u001b[39m.\u001b[39mtensor(scales[\u001b[39m0\u001b[39;49m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    126\u001b[0m lvl_max \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog2(torch\u001b[39m.\u001b[39mtensor(scales[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    128\u001b[0m map_levels \u001b[39m=\u001b[39m initLevelMapper(\n\u001b[1;32m    129\u001b[0m     \u001b[39mint\u001b[39m(lvl_min),\n\u001b[1;32m    130\u001b[0m     \u001b[39mint\u001b[39m(lvl_max),\n\u001b[1;32m    131\u001b[0m     canonical_scale\u001b[39m=\u001b[39mcanonical_scale,\n\u001b[1;32m    132\u001b[0m     canonical_level\u001b[39m=\u001b[39mcanonical_level,\n\u001b[1;32m    133\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_imgs = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "all_masks = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "all_imgs = np.array(all_imgs)\n",
    "all_masks = np.array(all_masks)\n",
    "train_img = all_imgs[train_index]\n",
    "train_mask = all_masks[train_index]\n",
    "val_img = all_imgs[test_index]\n",
    "val_mask = all_masks[test_index]\n",
    "dataset_train = PennFudanDataset(train_img, train_mask, get_transform(train=True))\n",
    "dataset_val = PennFudanDataset(val_img, val_mask, get_transform(train=False))\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=3, shuffle=True, num_workers=1, pin_memory=True, drop_last=True, collate_fn=utils.collate_fn)\n",
    "val_dl = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=1, pin_memory=True,collate_fn=utils.collate_fn)\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes=2)\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.00003)\n",
    "# optimizer = torch.optim.Adam(params, lr=0.0001, weight_decay=1e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "# set linear warmup scheduler, with constant learning rate after warmup\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,start_factor=0.01,\n",
    "                                            end_factor=1,\n",
    "                                            total_iters=10)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq=50)\n",
    "    evaluate(model, val_dl, device=device)\n",
    "    scheduler.step()\n",
    "    model_path = f'ckpts/fold_{0}_epoch{epoch}.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
