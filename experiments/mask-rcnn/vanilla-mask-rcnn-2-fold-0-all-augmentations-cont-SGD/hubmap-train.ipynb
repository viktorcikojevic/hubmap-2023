{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.158863Z",
     "iopub.status.busy": "2023-07-11T20:55:50.158344Z",
     "iopub.status.idle": "2023-07-11T20:55:50.649519Z",
     "shell.execute_reply": "2023-07-11T20:55:50.649187Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.append(\"detection-wheel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.651038Z",
     "iopub.status.busy": "2023-07-11T20:55:50.650891Z",
     "iopub.status.idle": "2023-07-11T20:55:50.652551Z",
     "shell.execute_reply": "2023-07-11T20:55:50.652375Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:50.653765Z",
     "iopub.status.busy": "2023-07-11T20:55:50.653677Z",
     "iopub.status.idle": "2023-07-11T20:55:51.175336Z",
     "shell.execute_reply": "2023-07-11T20:55:51.175011Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDatasetVal(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs, masks, transforms):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = imgs#sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "        self.masks = masks#sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = self.imgs[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        #masks = (mask == obj_ids[:, None, None])\n",
    "        #print((obj_ids[:, None, None]).shape)\n",
    "        #masks = mask == obj_ids[:, None, None]\n",
    "        masks = [np.where(mask== obj_ids[i, None, None],1,0) for i in range(len(obj_ids))]\n",
    "        masks = np.array(masks)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.nonzero(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        try:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            #print(area,area.shape,area.dtype)\n",
    "        except:\n",
    "            area = torch.tensor([[0],[0]])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        #print(masks.shape)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "from skimage.draw import polygon\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs, masks, mode='train'):\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = imgs\n",
    "        self.masks = masks\n",
    "\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "        labels_file = \"/home/viktor/Documents/kaggle/hubmap-2023/kaggle-data/polygons.jsonl\"\n",
    "        with open(labels_file, 'r') as json_file:\n",
    "            self.json_labels = [json.loads(line) for line in json_file]\n",
    "            \n",
    "        # get index for each 'id' \n",
    "        indices_map = {}\n",
    "        for indx, label in enumerate(self.json_labels):\n",
    "            indices_map[label['id']] = indx\n",
    "        \n",
    "        self.indices_map = indices_map\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.alb_transform = A.Compose([\n",
    "                \n",
    "                \n",
    "                    A.Rotate(limit=90, p=0.9),\n",
    "                    A.ShiftScaleRotate(p=0.5),\n",
    "                    A.RandomBrightnessContrast(p=0.2),\n",
    "                    A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.2),\n",
    "                    A.RandomGamma(p=0.2),\n",
    "                    # A.RandomCrop(height=256, width=256, p=0.5),\n",
    "                    A.RandomResizedCrop(height=512, width=512, p=0.5),\n",
    "                    A.Affine(p=0.5),\n",
    "                    A.Downscale(scale_min=0.1, scale_max=0.5, p=0.5),\n",
    "                    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, mask_fill_value=0, p=0.3),\n",
    "                    A.ShiftScaleRotate(shift_limit=0, scale_limit=0, p=0.3),\n",
    "                    \n",
    "                    \n",
    "                    A.Normalize(\n",
    "                        mean= [0] * 3,\n",
    "                        std= [1] * 3\n",
    "                    ),\n",
    "                \n",
    "                \n",
    "                    ToTensorV2(transpose_mask=True),\n",
    "                ])\n",
    "        else:\n",
    "            self.alb_transform = A.Compose([\n",
    "                    \n",
    "                    A.Normalize(\n",
    "                        mean= [0] * 3,\n",
    "                        std= [1] * 3\n",
    "                    ),\n",
    "                \n",
    "                \n",
    "                    ToTensorV2(transpose_mask=True),\n",
    "                ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        num_objs = 0\n",
    "        while num_objs == 0:\n",
    "        \n",
    "            idx = np.random.randint(0, len(self.imgs))\n",
    "            \n",
    "            # load images and masks\n",
    "            img_path = self.imgs[idx]\n",
    "            mask_path = self.masks[idx] # '/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/0006ff2aa7cd_mask.png'\n",
    "            \n",
    "            \n",
    "            \n",
    "            # load image\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = np.array(img)\n",
    "            \n",
    "            # load mask, but first get the id\n",
    "            mask_id = mask_path.split('/')[-1].split('_')[0]\n",
    "            \n",
    "            mask = np.zeros((512, 512), dtype=np.float32)\n",
    "            \n",
    "            \n",
    "            mask_id_indx = self.indices_map[mask_id]\n",
    "            for annot in self.json_labels[mask_id_indx]['annotations']:\n",
    "                cords = annot['coordinates']\n",
    "                if annot['type'] == \"blood_vessel\":\n",
    "                    for cord in cords:\n",
    "                        rr, cc = polygon(np.array([i[1] for i in cord]), np.asarray([i[0] for i in cord]))\n",
    "                        mask[rr, cc] = 1\n",
    "            \n",
    "        \n",
    "            # ---------------- Augmentations ----------------\n",
    "            \n",
    "            \n",
    "            \n",
    "            # albumentations\n",
    "            transformed = self.alb_transform(image=img, mask=mask)\n",
    "            \n",
    "            img = transformed['image']\n",
    "            mask = transformed['mask'].numpy()\n",
    "            \n",
    "            # return img, mask\n",
    "            \n",
    "            # print(img.shape, mask.shape)\n",
    "            mask_uint8 = np.where(mask > 0.5, 1, 0).astype(np.uint8) * 255\n",
    "                \n",
    "            num_outputs, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_uint8)\n",
    "            label_masks = [labels == i for i in range(num_outputs)]\n",
    "            masks = []\n",
    "            for m in label_masks:\n",
    "                mask_m = mask * m\n",
    "                if np.sum(mask_m) > 80:\n",
    "                    masks.append(mask_m)\n",
    "                    \n",
    "            \n",
    "            # -----------------------------------------------\n",
    "            \n",
    "            # get bounding box coordinates for each mask\n",
    "            num_objs = len(masks)\n",
    "            boxes = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        for i in range(num_objs):\n",
    "            pos = np.nonzero(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            #print(area,area.shape,area.dtype)\n",
    "        except:\n",
    "            area = torch.tensor([[0],[0]])\n",
    "        \n",
    "        \n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target['num_objs'] = torch.tensor(num_objs)      \n",
    "        target[\"image_id\"] = image_id  \n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "                \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "train_mask = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "dataset_train = PennFudanDataset(train_img, train_mask,  mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = dataset_train[0]\n",
    "\n",
    "\n",
    "\n",
    "masks = [m for m in target['masks'].numpy()]\n",
    "print(len(masks))\n",
    "\n",
    "plt.imshow(np.sum(masks, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.176860Z",
     "iopub.status.busy": "2023-07-11T20:55:51.176708Z",
     "iopub.status.idle": "2023-07-11T20:55:51.228454Z",
     "shell.execute_reply": "2023-07-11T20:55:51.228169Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import list_models\n",
    "detection_models = list_models(module=torchvision.models.detection)\n",
    "detection_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.243653Z",
     "iopub.status.busy": "2023-07-11T20:55:51.243517Z",
     "iopub.status.idle": "2023-07-11T20:55:51.245790Z",
     "shell.execute_reply": "2023-07-11T20:55:51.245599Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\", weights_backbone=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomIoUCrop())\n",
    "        transforms.append(T.RandomZoomOut())\n",
    "        transforms.append(T.RandomPhotometricDistort())\n",
    "        transforms.append(T.ScaleJitter())\n",
    "        transforms.append(T.RandomShortestSize())\n",
    "        \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.250040Z",
     "iopub.status.busy": "2023-07-11T20:55:51.249919Z",
     "iopub.status.idle": "2023-07-11T20:55:51.252379Z",
     "shell.execute_reply": "2023-07-11T20:55:51.252197Z"
    }
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.253503Z",
     "iopub.status.busy": "2023-07-11T20:55:51.253387Z",
     "iopub.status.idle": "2023-07-11T20:55:51.262727Z",
     "shell.execute_reply": "2023-07-11T20:55:51.262523Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.263931Z",
     "iopub.status.busy": "2023-07-11T20:55:51.263788Z",
     "iopub.status.idle": "2023-07-11T20:55:51.266884Z",
     "shell.execute_reply": "2023-07-11T20:55:51.266711Z"
    }
   },
   "outputs": [],
   "source": [
    "n_imgs = len(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*'))\n",
    "n_imgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Create a custom function to log output\n",
    "def log_output(text):\n",
    "    with open('output.log', 'a') as f:\n",
    "        f.write(text)\n",
    "\n",
    "# Redirect stdout to the custom log function\n",
    "sys.stdout.write = log_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T20:55:51.268015Z",
     "iopub.status.busy": "2023-07-11T20:55:51.267921Z",
     "iopub.status.idle": "2023-07-11T20:55:51.269472Z",
     "shell.execute_reply": "2023-07-11T20:55:51.269301Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(range(n_imgs))):\n",
    "    if i!=0: continue\n",
    "    all_imgs = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/image/*.png'))\n",
    "    all_masks = sorted(glob.glob('/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/new-dataset/train/mask/*.png'))\n",
    "    \n",
    "    all_imgs = np.array(all_imgs)\n",
    "    all_masks = np.array(all_masks)\n",
    "    train_img = all_imgs[train_index]\n",
    "    train_mask = all_masks[train_index]\n",
    "    \n",
    "    \n",
    "    # train_img = train_img\n",
    "    \n",
    "    val_img = all_imgs[test_index]\n",
    "    val_mask = all_masks[test_index]\n",
    "    dataset_train = PennFudanDataset(train_img, train_mask, mode='train')\n",
    "    dataset_val = PennFudanDatasetVal(val_img, val_mask, get_transform(train=False))\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=4, shuffle=True, num_workers=os.cpu_count(), pin_memory=True, drop_last=True, collate_fn=utils.collate_fn)\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True,collate_fn=utils.collate_fn)\n",
    "                                                        #os.cpu_count()\n",
    "    \n",
    "    \n",
    "    model = get_model_instance_segmentation(num_classes=2)\n",
    "    \n",
    "    weights_path = \"/home/viktor/Documents/kaggle/hubmap-2023/experiments/mask-rcnn/vanilla-mask-rcnn-2-fold-0-all-augmentations/ckpts/fold_0_epoch164.pth\"\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.02)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    # set linear warmup scheduler, with constant learning rate after warmup\n",
    "    \n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "    #                                                 base_lr=2e-5, \n",
    "    #                                                 max_lr=3e-3, \n",
    "    #                                                 step_size_up=40, \n",
    "    #                                                 step_size_down=40, \n",
    "    #                                                 mode='triangular2', \n",
    "    #                                                 cycle_momentum=False)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,start_factor=0.01,\n",
    "    #                                         end_factor=1,\n",
    "    #                                         total_iters=10)\n",
    "\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq=50)\n",
    "        evaluate(model, val_dl, device=device)\n",
    "        # scheduler.step()\n",
    "        model_path = f'ckpts/fold_{i}_epoch{epoch}.pth' \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
